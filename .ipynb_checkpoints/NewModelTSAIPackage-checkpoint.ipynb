{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0957a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET FEATURE FILES FROM GDRIVE \n",
    "# COLAB ONLY - MOVE FEATURE FILES TO WORKING DIRECTORY\n",
    "# \n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\") and not os.path.exists(WORKING_DIR):\n",
    "  !mkdir '/content/asl-work'\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   # shutil.copy(f\"/content/drive/MyDrive/GaggleSignLang/feature_data{FRAMES_OUT}.npy\", f\"{WORKING_DIR}\")\n",
    "    shutil.copy(f\"/content/drive/MyDrive/GaggleSignLang/feature_labels.npy\", f\"{WORKING_DIR}\")\n",
    "    shutil.copy(f\"/content/drive/MyDrive/GaggleSignLang/sign_to_prediction_index_map.json\", f\"{WORKING_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ac5c251a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T09:22:00.056762Z",
     "iopub.status.busy": "2023-04-27T09:22:00.056345Z",
     "iopub.status.idle": "2023-04-27T09:22:01.788737Z",
     "shell.execute_reply": "2023-04-27T09:22:01.787457Z",
     "shell.execute_reply.started": "2023-04-27T09:22:00.056725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Jupiter  MacOS\n",
    "BASE_DIR = \"/Users/johnhanratty/ASLtest/asl-signs\"  #\"/Users/johnhanratty/ASLtest/asl-signs\"\n",
    "WORKING_DIR = \"/Users/johnhanratty/ASLtest\"\n",
    "ARCHIVE_DIR = \"/Users/johnhanratty/ASLtest\"\n",
    "MODELS_DIR = \"/Users/johnhanratty/ASLtest/models\"\n",
    "\n",
    "# !pip install nb_black --quiet\n",
    "# %load_ext lab_black\n",
    "\n",
    "# Colab\n",
    "# BASE_DIR = \"/content/asl-signs\"   #\"/content/drive/MyDrive/GaggleSignLang/asl-signs\"\n",
    "# WORKING_DIR = \"/content/asl-work\"\n",
    "# ARCHIVE_DIR = \"/content/drive/MyDrive/GaggleSignLang\"\n",
    "# !pip install nb_black --quiet\n",
    "# print('-----ok')\n",
    "# %load_ext nb_black\n",
    "\n",
    "# KAGGLE\n",
    "# BASE_DIR = \"/kaggle/input/asl-signs\"\n",
    "# WORKING_DIR = \"/kaggle/working\"\n",
    "# ARCHIVE_DIR = \"/kaggle/working\"\n",
    "# MODEL_DIR  = \"/kaggle/working\"\n",
    "# !pip install nb_black --quiet --root-user-action=ignore\n",
    "# %load_ext lab_black\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from random import seed, sample\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "LANDMARK_FILES_DIR = f'{BASE_DIR}/train_landmark_files'\n",
    "TRAIN_FILE = f\"{BASE_DIR}/train.csv\"\n",
    "\n",
    "FRAMES_OUT = 32 # 16\n",
    "PTS_IN_FRAME = 345\n",
    "DIMC = [0,1,2]\n",
    "DIMS = len(DIMC)\n",
    "WORKERS = 0   # dataoader work var  0 for MAC, 4 for online\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a41802f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T09:22:06.975674Z",
     "iopub.status.busy": "2023-04-27T09:22:06.975115Z",
     "iopub.status.idle": "2023-04-27T09:22:06.982263Z",
     "shell.execute_reply": "2023-04-27T09:22:06.981046Z",
     "shell.execute_reply.started": "2023-04-27T09:22:06.975635Z"
    }
   },
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a6048e13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T09:22:24.657013Z",
     "iopub.status.busy": "2023-04-27T09:22:24.656552Z",
     "iopub.status.idle": "2023-04-27T09:22:24.672857Z",
     "shell.execute_reply": "2023-04-27T09:22:24.671586Z",
     "shell.execute_reply.started": "2023-04-27T09:22:24.656974Z"
    }
   },
   "outputs": [],
   "source": [
    "#FEATUREGEN MODEL\n",
    "ROWS_PER_FRAME = 543  # combined face, lefth, pose, righth\n",
    "\n",
    "# FILTER FEATURES IN EACH FRAME  - FACE, POSE & HANDs\n",
    "class FeatureGen(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureGen, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # FILTER TO SPECIFIED FRAMES (FRAMES_OUT)\n",
    "        seed(24)\n",
    "        x = np.array(x)\n",
    "        n_frames = x.shape[0]\n",
    "        # Trim to # of frames to FRAMES_OUT\n",
    "        if n_frames > FRAMES_OUT:\n",
    "            idx = sorted((sample(range(0, n_frames), FRAMES_OUT)))\n",
    "            x=x[idx,:,:]\n",
    "        n_frames = x.shape[0]\n",
    "        # FLATTENING ROWS BY TYPE and CONCATENATING TO ONE ROW PER FRAME 3D (XYZ)\n",
    "        # INPUT NUMPY, TORCH OUTPUT\n",
    "\n",
    "        # Grab data type (e.g. one point on hand) by selecting rows for each frame\n",
    "        # face_x = x[:,:468,:].contiguous().view(-1, 468*3)\n",
    "        lips_idx = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291, 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308, 95, 88, 178, 87, 14, 317, 402, 318, 324, 146, 91, 181, 84, 17, 314, 405, 321, 375]\n",
    "        lips_x = x[:, lips_idx,:].reshape(-1, len(lips_idx)*3)\n",
    "        lefth_x = x[:,468:489,:].reshape(-1, 21*3)\n",
    "        pose_x = x[:,489:522,:].reshape(-1, 33*3)\n",
    "        righth_x = x[:,522:,:].reshape(-1, 21*3)\n",
    "\n",
    "        if np.isnan(lefth_x).sum() < np.isnan(righth_x).sum():\n",
    "            prime_x = lefth_x\n",
    "            second_x = righth_x\n",
    "        else:\n",
    "            prime_x = righth_x.reshape(righth_x.shape[0], -1, DIMS)\n",
    "            prime_x[:,:,0] = np.add(np.nanmax(prime_x[:,:,0], axis=1).reshape(-1,1),\n",
    "                                    prime_x[:, :, 0])\n",
    "            prime_x = prime_x.reshape(prime_x.shape[0],-1)\n",
    "            \n",
    "            second_x = lefth_x.reshape(lefth_x.shape[0], -1, DIMS)\n",
    "            second_x[:,:,0] = np.subtract(np.nanmax(second_x[:,:,0], axis=1).reshape(-1,1),\n",
    "                                          second_x[:, :, 0])\n",
    "            second_x = second_x.reshape(second_x.shape[0],-1)\n",
    "            \n",
    "        # ?? remove empty frames ???\n",
    "        \n",
    "        # flatten types into one row per frame\n",
    "        xfeat = np.full([FRAMES_OUT, PTS_IN_FRAME], np.nan)\n",
    "        offset = (FRAMES_OUT - n_frames) // 2  # center frames in output data in each frame in video\n",
    "        xfeat[offset:n_frames+offset,:] = np.concatenate([lips_x, prime_x, pose_x, second_x], axis=1)  # concatenate types\n",
    "        \n",
    "        \n",
    "        def distDiff(ds, ref, pts):\n",
    "            ds = ds.reshape(ds.shape[0],  -1, DIMS)\n",
    "            d = np.hstack([np.nanmean(ds[:, pts, :], axis=0), \n",
    "                           np.nanmedian(ds[:, pts, :], axis=0), \n",
    "                           np.nanmax(ds[:, pts, :], axis=0), \n",
    "                           np.nanmin(ds[:, pts, :], axis=0),\n",
    "                           np.nanvar(ds[:, pts, :], axis=0)\n",
    "                           ]) \n",
    "            d = d.reshape(1, -1) \n",
    "            # NORMALIZE\n",
    "           # d = (d - np.nanmean(d, keepdims=True)) / np.nanstd(d, keepdims=True) # -1 to 1\n",
    "            d = np.nan_to_num(d, copy=False)  # replace NaN after normalization\n",
    "            return d\n",
    "        \n",
    "        d1 = distDiff(xfeat, 40, [44, 48, 52, 56, 60, 43, 46, 50, 54, 58])\n",
    "        d2 = distDiff(xfeat, 40, [98, 102, 106, 110, 114, 97, 102, 106, 110, 114])\n",
    "        d3 = distDiff(xfeat, 60, [73, 80, 81, 76, 77, 68, 69, 70, 71, 75, 74])\n",
    "        d4 = distDiff(xfeat, 5,  [0, 4, 8, 12, 16, 20, 24, 28, 32, 36])\n",
    "        return d1, d2, d3, d4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dd1fee39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T10:13:04.044426Z",
     "iopub.status.busy": "2023-04-27T10:13:04.043547Z",
     "iopub.status.idle": "2023-04-27T10:16:04.541551Z",
     "shell.execute_reply": "2023-04-27T10:16:04.540255Z",
     "shell.execute_reply.started": "2023-04-27T10:13:04.044379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert&Save (94477, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 94477/94477 [13:29<00:00, 116.71it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## PROCESS EACH ROW (ONE PARQUET PER ROW)\n",
    "def convert_row(row):\n",
    "    x = load_relevant_data_subset(os.path.join(BASE_DIR, row[1].path))\n",
    "    x1,x2,x3,x4 = feature_converter(torch.tensor(x))\n",
    "    return x1,x2,x3,x4, row[1].label\n",
    "\n",
    "## LOOP THROUGH PARQUET FILES LISTED IN TRAIN FILE\n",
    "##  SAVE RESULTS \n",
    "def convert_and_save_data():\n",
    "    label_map = json.load(open(f\"{BASE_DIR}/sign_to_prediction_index_map.json\", \"r\"))\n",
    "    df = pd.read_csv(TRAIN_FILE)\n",
    "    df['label'] = df['sign'].map(label_map)\n",
    "    \n",
    "    print(\"Convert&Save\", df.shape)\n",
    "    #### FOR TESTING #################\n",
    "    #df = df[0:20]\n",
    "    ##################################\n",
    "\n",
    "    npdata1 = np.zeros((df.shape[0], 150))\n",
    "    npdata2 = np.zeros((df.shape[0], 150))\n",
    "    npdata3 = np.zeros((df.shape[0], 165))\n",
    "    npdata4 = np.zeros((df.shape[0], 150))\n",
    "\n",
    "    nplabels = np.zeros(df.shape[0])\n",
    "    \n",
    "    results = map(convert_row, df.iterrows())\n",
    "    for i, (x1,x2,x3,x4,y) in tqdm(enumerate(results), total=df.shape[0]):\n",
    "            npdata1[i,:] = x1\n",
    "            npdata2[i,:] = x2\n",
    "            npdata3[i,:] = x3\n",
    "            npdata4[i,:] = x4\n",
    "            nplabels[i] = y\n",
    "    return npdata1, npdata2, npdata3, npdata4, nplabels\n",
    " \n",
    "\n",
    "feature_converter = FeatureGen()\n",
    "d1, d2, d3, d4, datay = convert_and_save_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "704dc178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T11:16:34.423269Z",
     "iopub.status.busy": "2023-04-27T11:16:34.422846Z",
     "iopub.status.idle": "2023-04-27T11:16:34.445881Z",
     "shell.execute_reply": "2023-04-27T11:16:34.444872Z",
     "shell.execute_reply.started": "2023-04-27T11:16:34.423234Z"
    }
   },
   "outputs": [],
   "source": [
    "#MODEL\n",
    "### NEW SEPARATED INPUTS\n",
    "class ASLData(Dataset):\n",
    "    def __init__(self,d1,d2,d3,d4,datay):\n",
    "        self.d1 = d1\n",
    "        self.d2 = d2\n",
    "        self.d3 = d3\n",
    "        self.d4 = d4\n",
    "        self.datay = datay\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.d1[index, :], self.d2[index, :],self.d3[index, :],self.d4[index, :], self.datay[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datay)\n",
    "\n",
    "# https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab\n",
    "class ASLModel(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(ASLModel, self).__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        L1OUT = 512  #1024 was ok\n",
    "        L2OUT = 512\n",
    "\n",
    "        self.layer_ph = nn.Linear(150, L1OUT)\n",
    "        self.batchnorm_ph = nn.BatchNorm1d(L1OUT)\n",
    "        \n",
    "        self.layer_sh = nn.Linear(150, L1OUT)\n",
    "        self.batchnorm_sh = nn.BatchNorm1d(L1OUT)\n",
    " \n",
    "        self.layer_po = nn.Linear(165, L1OUT)\n",
    "        self.batchnorm_po = nn.BatchNorm1d(L1OUT)\n",
    " \n",
    "        self.layer_li = nn.Linear(150, L1OUT)\n",
    "        self.batchnorm_li = nn.BatchNorm1d(L1OUT) \n",
    " \n",
    "        self.layer1 = nn.Linear(4*L1OUT, L2OUT)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(L2OUT)\n",
    "\n",
    "        self.layerFC = nn.Linear(L2OUT, 250)\n",
    "        self.softmax = nn.Softmax()\n",
    " \n",
    "        \n",
    "    def forward(self, phand, shand, pose, lips):\n",
    "        ph = self.flatten(torch.tensor(phand).float()) \n",
    "        ph = self.layer_ph(ph)\n",
    "        ph = self.batchnorm_ph(ph)\n",
    "        ph = self.relu(ph)\n",
    "        ph = self.dropout(ph)\n",
    "\n",
    "        sh = self.flatten(torch.tensor(shand).float())       \n",
    "        sh = self.layer_sh(sh)\n",
    "        sh = self.batchnorm_sh(sh)\n",
    "        sh = self.relu(sh)\n",
    "        sh = self.dropout(sh)\n",
    "       \n",
    "        po = self.flatten(torch.tensor(pose).float())       \n",
    "        po = self.layer_po(po)\n",
    "        po = self.batchnorm_po(po)\n",
    "        po = self.relu(po)\n",
    "        po = self.dropout(po)\n",
    "        \n",
    "        li = self.flatten(torch.tensor(lips).float())       \n",
    "        li = self.layer_li(li)\n",
    "        li = self.batchnorm_li(li)\n",
    "        li = self.relu(li)\n",
    "        li = self.dropout(li)\n",
    "\n",
    "        x = torch.cat((ph.view(ph.size(0), -1),\n",
    "                       sh.view(sh.size(0), -1),\n",
    "                       po.view(po.size(0), -1),\n",
    "                       li.view(li.size(0), -1)), dim=1)\n",
    "        # x = self.batchnorm0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layerFC(x)\n",
    "       # x = self.softmax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8f30afac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T11:16:40.228537Z",
     "iopub.status.busy": "2023-04-27T11:16:40.227418Z",
     "iopub.status.idle": "2023-04-27T11:17:22.481778Z",
     "shell.execute_reply": "2023-04-27T11:17:22.480359Z",
     "shell.execute_reply.started": "2023-04-27T11:16:40.228485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++using CPU++++\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:0 > Train Loss: 3.5221, Train Acc: 0.2184\n",
      "Epoch:0 > Val Loss: 2.6717, Val Acc: 0.3625\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:1 > Train Loss: 2.3976, Train Acc: 0.4125\n",
      "Epoch:1 > Val Loss: 2.2567, Val Acc: 0.4536\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:2 > Train Loss: 2.0461, Train Acc: 0.4871\n",
      "Epoch:2 > Val Loss: 2.0997, Val Acc: 0.4855\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:3 > Train Loss: 1.8092, Train Acc: 0.5410\n",
      "Epoch:3 > Val Loss: 1.8982, Val Acc: 0.5301\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:4 > Train Loss: 1.6479, Train Acc: 0.5767\n",
      "Epoch:4 > Val Loss: 1.6472, Val Acc: 0.5956\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:5 > Train Loss: 1.5183, Train Acc: 0.6074\n",
      "Epoch:5 > Val Loss: 1.5415, Val Acc: 0.6215\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:6 > Train Loss: 1.4183, Train Acc: 0.6299\n",
      "Epoch:6 > Val Loss: 1.4450, Val Acc: 0.6464\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:7 > Train Loss: 1.3366, Train Acc: 0.6477\n",
      "Epoch:7 > Val Loss: 1.3906, Val Acc: 0.6595\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:8 > Train Loss: 1.2760, Train Acc: 0.6639\n",
      "Epoch:8 > Val Loss: 1.3832, Val Acc: 0.6608\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:9 > Train Loss: 1.2196, Train Acc: 0.6766\n",
      "Epoch:9 > Val Loss: 1.3340, Val Acc: 0.6740\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:10 > Train Loss: 1.1837, Train Acc: 0.6854\n",
      "Epoch:10 > Val Loss: 1.3293, Val Acc: 0.6749\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:11 > Train Loss: 1.1434, Train Acc: 0.6950\n",
      "Epoch:11 > Val Loss: 1.3014, Val Acc: 0.6824\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:12 > Train Loss: 1.1154, Train Acc: 0.7008\n",
      "Epoch:12 > Val Loss: 1.2921, Val Acc: 0.6849\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:13 > Train Loss: 1.0932, Train Acc: 0.7042\n",
      "Epoch:13 > Val Loss: 1.2959, Val Acc: 0.6854\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:14 > Train Loss: 1.0715, Train Acc: 0.7131\n",
      "Epoch:14 > Val Loss: 1.2895, Val Acc: 0.6859\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:15 > Train Loss: 1.0547, Train Acc: 0.7148\n",
      "Epoch:15 > Val Loss: 1.2716, Val Acc: 0.6908\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:16 > Train Loss: 1.0479, Train Acc: 0.7175\n",
      "Epoch:16 > Val Loss: 1.2726, Val Acc: 0.6921\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:17 > Train Loss: 1.0379, Train Acc: 0.7203\n",
      "Epoch:17 > Val Loss: 1.2693, Val Acc: 0.6930\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:18 > Train Loss: 1.0278, Train Acc: 0.7224\n",
      "Epoch:18 > Val Loss: 1.2634, Val Acc: 0.6928\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:19 > Train Loss: 1.0208, Train Acc: 0.7222\n",
      "Epoch:19 > Val Loss: 1.2695, Val Acc: 0.6929\n",
      "==================================================\n",
      "#### ELAPSED TIME: 649.6243511879984\n"
     ]
    }
   ],
   "source": [
    "## MULTI TRAINING\n",
    "# !!! TRAINING DOES NOT RUN ON MAC OS - (cuda)\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  print(\"++++using GPU++++\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print(\"++++using CPU++++\")\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "#datax = datax.reshape(datax.shape[0],datax.shape[1], -1) #.swapaxes(1,2)\n",
    "#datax = torch.tensor(datax)  # Convert to Torch Tensor\n",
    "d1 = torch.tensor(d1)  # Convert to Torch Tensor\n",
    "d2 = torch.tensor(d2)  # Convert to Torch Tensor\n",
    "d3 = torch.tensor(d3)  # Convert to Torch Tensor\n",
    "d4 = torch.tensor(d4)  # Convert to Torch Tensor\n",
    "traind1, testd1, traind2, testd2, traind3, tesdt3,traind4,testd4, trainy, testy = train_test_split(d1, d2, d3, d4, datay, test_size=0.15, random_state=42)\n",
    "train_data = ASLData(traind1, traind2, traind3, traind4, trainy)\n",
    "valid_data = ASLData(testd1, testd2, tesdt3, testd4, testy)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=WORKERS, shuffle=True)\n",
    "val_loader = DataLoader(valid_data, batch_size=BATCH_SIZE, num_workers=WORKERS, shuffle=False)\n",
    "model = ASLModel(0.2).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "sched = torch.optim.lr_scheduler.StepLR(opt, step_size=300, gamma=0.95)\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss_sum = 0.\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_bar = train_loader\n",
    "    for x1,x2,x3,x4,y in train_bar:\n",
    "        x1 = torch.Tensor(x1).float().to(device)\n",
    "        x2 = torch.Tensor(x2).float().to(device)\n",
    "        x3 = torch.Tensor(x3).float().to(device)\n",
    "        x4 = torch.Tensor(x4).float().to(device)\n",
    "\n",
    "        y = torch.Tensor(y).long().to(device) \n",
    "        y_pred = model(x1,x2,x3,x4)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        train_loss_sum += loss.item()\n",
    "        train_correct += np.sum((np.argmax(y_pred.detach().cpu().numpy(), axis=1) == y.cpu().numpy()))\n",
    "        train_total += 1\n",
    "        sched.step()\n",
    "        \n",
    "    val_loss_sum = 0.\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    model.eval()\n",
    "    for x1,x2,x3,x4,y in val_loader:\n",
    "        x1 = torch.Tensor(x1).float().to(device)\n",
    "        x2 = torch.Tensor(x2).float().to(device)\n",
    "        x3 = torch.Tensor(x3).float().to(device)\n",
    "        x4 = torch.Tensor(x4).float().to(device)\n",
    "        y = torch.Tensor(y).long().to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x1,x2,x3,x4)\n",
    "            loss = criterion(y_pred, y)\n",
    "            val_loss_sum += loss.item()\n",
    "            val_correct += np.sum((np.argmax(y_pred.cpu().numpy(), axis=1) == y.cpu().numpy()))\n",
    "            val_total += 1\n",
    "    print(f\"DIM={DIMS} FRAMES={FRAMES_OUT}, FEAT={PTS_IN_FRAME}\")                          \n",
    "    print(f\"Epoch:{i} > Train Loss: {(train_loss_sum/train_total):.04f}, Train Acc: {train_correct/len(train_data):0.04f}\")\n",
    "    print(f\"Epoch:{i} > Val Loss: {(val_loss_sum/val_total):.04f}, Val Acc: {val_correct/len(valid_data):0.04f}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Save the pytorch model\n",
    "PATH = f\"{ARCHIVE_DIR}/models/modelccn{FRAMES_OUT}flat.sd\"\n",
    "#torch.save(model.state_dict(), PATH)\n",
    "\n",
    "print(\"#### ELAPSED TIME:\", time.perf_counter()-start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49046438",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3540859849.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[32], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ========Full Data - no add Norm, Centered ========\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "========Full Data - no add Norm, Centered ========\n",
    "DIM=3 FRAMES=32, FEAT=345\n",
    "Epoch:38 > Train Loss: 0.9867, Train Acc: 0.7321\n",
    "Epoch:38 > Val Loss: 1.2608, Val Acc: 0.6988\n",
    "==================================================\n",
    "\n",
    "========Full Data - no additional Normalizeation/Centering=====================================\n",
    "DIM=3 FRAMES=32, FEAT=345\n",
    "Epoch:39 > Train Loss: 1.4974, Train Acc: 0.6059\n",
    "Epoch:39 > Val Loss: 1.7298, Val Acc: 0.5839\n",
    "==================================================\n",
    "\n",
    "=====Full no reverse, centered =============================================\n",
    "DIM=3 FRAMES=32, FEAT=345\n",
    "Epoch:39 > Train Loss: 1.2104, Train Acc: 0.6745\n",
    "Epoch:39 > Val Loss: 1.4204, Val Acc: 0.6515\n",
    "==================================================\n",
    "# Additional Normalize and Nan Step  0.6100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3cd9f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE PYTORCH MODELS\n",
    "class AModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AModel, self).__init__()\n",
    "        \n",
    "        self.InputFormat = feature_converter\n",
    "        self.InferModel = model\n",
    "        self.InferModel.eval()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1,x2,x3,x4 = self.InputFormat(x)\n",
    "        pred = self.InferModel(x1,x2,x3,x4)\n",
    "        return pred\n",
    "\n",
    "mod = AModel()\n",
    "PATH = f\"{ARCHIVE_DIR}/models/modelccn{FRAMES_OUT}test.pt\"\n",
    "torch.save(mod, PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "885e7ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94477, 5)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_map = json.load(open(f\"{BASE_DIR}/sign_to_prediction_index_map.json\", \"r\"))\n",
    "df = pd.read_csv(TRAIN_FILE)\n",
    "df['label'] = df['sign'].map(label_map)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1648f34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/johnhanratty/ASLtest/asl-signs/train_landmark_files/36257/1083470917.parquet'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(BASE_DIR, d['path'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "557b3a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth: 2022    56\n",
      "Name: label, dtype: int64 2022    dance\n",
      "Name: sign, dtype: object prediction= 56\n"
     ]
    }
   ],
   "source": [
    "Infmodel = torch.load(PATH)\n",
    "\n",
    "d = df[2022:2023]\n",
    "\n",
    "x = load_relevant_data_subset(os.path.join(BASE_DIR, d['path'].item()))\n",
    "pred = Infmodel(x)\n",
    "\n",
    "print(\"truth:\", d.label, d.sign, \"prediction=\", np.argmax(pred.detach().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbbbe3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
