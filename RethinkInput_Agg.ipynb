{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Mediapipe Specs\n",
        "https://google.github.io/mediapipe/solutions/pose.html \n",
        "\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=download&id=1nSdlOv09Isye_wtDIfHQlrJbhPqWhx3M\" width=\"400\">\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=download&id=1nW9OZDMGXQEv_KHRPJ5SveOjLP0O_CfA\" width=\"400\">\n",
        "\n",
        "\n",
        "\n",
        "Face\n",
        "https://github.com/google/mediapipe/blob/a908d668c730da128dfa8d9f6bd25d519d006692/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png \n"
      ],
      "metadata": {
        "id": "9ZedVyPyRP8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hhTKwwaiev4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HANDIDX = {\"WRIST\":0, \"THUMB_CMC\":1, \"THUMB_MCP\":2,     \"THUMB_IP\":3,          \"THUMB_TIP\":4,\n",
        "        \"INDEX_FINGER_MCP\":5, \"INDEX_FINGER_PIP\":6,  \"INDEX_FINGER_DIP\":7,  \"INDEX_FINGER_TIP\":8,\n",
        "        \"MIDDLE_FINGER_MCP\":9,\"MIDDLE_FINGER_PIP\":10,\"MIDDLE_FINGER_DIP\":11,\"MIDDLE_FINGER_TIP\":12,\n",
        "        \"RING_FINGER_MCP\":13, \"RING_FINGER_PIP\":14,  \"RING_FINGER_DIP\":15,  \"RING_FINGER_TIP\":16,\n",
        "        \"PINKY_FINGER_MCP\":17,\"PINKY_FINGER_PIP\":18,  \"PINKY_FINGER_DIP\":19,\"PINKY_FINGER_TIP\":20}\n",
        "\n",
        "POSEIDX = {\"NOSE\":0,        \"LEFT_EYE_INNER\":1,\"LEFT_EYE\":2,   \"LEFT_EYE_OUTER\":3, \"RIGHT_EYE_INNER\":4, \"RIGHT_EYE\":5,      \"RIGHT_EYE_OUTER\":6,\n",
        "        \"LEFT_EAR\":7,    \"RIGHT_EAR\":8,     \"MOUTH_LEFT\":9, \"MOUTH_RIGHT\":10,   \"LEFT_SHOULDER\":11,  \"RIGHT_SHOULDER\":12,\n",
        "        \"LEFT_ELBOW\":13, \"RIGHT_ELBOW\":14,  \"LEFT_WRIST\":15, \"RIGHT_WRIST\":16, \n",
        "        \"LEFT_PINKY\":17, \"RIGHT_PINKY\":18,  \"LEFT_INDEX\":19,\"RIGHT_INDEX\":20,   \"RIGHT_THUMB\":21,    \"LEFT_THUMB\":22,\n",
        "        \"LEFT_HIP\":23,   \"RIGHT_HIP\":24,    \"LEFT_KNEE\":25, \"RIGHT_KNEE\":26,    \"LEFT_ANKLE\":27,     \"RIGHT_ANKLE\":28,\n",
        "        \"LEFT_HEEL\":29,  \"RIGHT_HEEL\":30,   \"LEFT_FOOT_INDEX\":31, \"RIGHT_FOOT_INDEX\":32}\n",
        "\n",
        "LIPSIDX = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291, 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308, 95, 88, 178, 87, 14, 317, 402, 318, 324, 146, 91, 181, 84, 17, 314, 405, 321, 375]\n",
        "LIPSIDX_sm = [185, 39, 0,  269,  409,  78,  80,  82,  312,  310,  308,  88,  87,  317,  318,  146,  181,  17,  405, 375]\n",
        "\n",
        "\n",
        "# need to rerun parquet processing to grab data points\n",
        "#LIPSIDX1 = [12, 268,271,272,407, 293, 325,319,403,316,15,86,179,89,96,62, 183,42,41,38]\n",
        "\n",
        "METALEN = len(LIPSIDX) + len(POSEIDX) + 2 * len(HANDIDX)\n",
        "\n",
        "def PRIM_HAND(idx): return(40 + HANDIDX[idx])\n",
        "def POSE(idx):  return(40 + 21 + POSEIDX[idx])\n",
        "def SEC_HAND(idx):  return(40 + 21 + 33 + HANDIDX[idx])\n",
        "\n",
        "print(POSE(\"NOSE\"))\n",
        "print(SEC_HAND(\"WRIST\"))\n",
        "METALEN\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy6apaHkeyTR",
        "outputId": "be089710-dcd8-4213-b5e5-81621e8d87dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61\n",
            "94\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "115"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr45cYjhsA6K",
        "outputId": "9b899456-070b-41b7-f7ae-570f2d66be7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Jupiter  MacOS\n",
        "# BASE_DIR = \"/Users/johnhanratty/Library/CloudStorage/OneDrive-Personal/IRMA_GIT/Kaggle_SignLanguage/asl-signs\"\n",
        "# WORKING_DIR = BASE_DIR\n",
        "# !pip install nb_black --quiet\n",
        "# %load_ext lab_black\n",
        "\n",
        "# Colab\n",
        "BASE_DIR = \"/content/asl-signs\"   #\"/content/drive/MyDrive/GaggleSignLang/asl-signs\"\n",
        "WORKING_DIR = \"/content/asl-work\"\n",
        "ARCHIVE_DIR = \"/content/drive/MyDrive/GaggleSignLang\"\n",
        "# !pip install nb_black --quiet\n",
        "# print('-----ok')\n",
        "# %load_ext nb_black\n",
        "\n",
        "# KAGGLE\n",
        "# BASE_DIR = \"/kaggle/input/asl-signs\"\n",
        "# WORKING_DIR = \"/kaggle/working/\"\n",
        "# !pip install nb_black --quiet --root-user-action=ignore\n",
        "# %load_ext lab_black\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "LANDMARK_FILES_DIR = f'{BASE_DIR}/train_landmark_files'\n",
        "TRAIN_FILE = f\"{BASE_DIR}/train.csv\"\n",
        "\n",
        "FRAMES_OUT = 64 #16 # 16\n",
        "PTS_IN_FRAME = 115\n",
        "DIMC = [0,1,2]\n",
        "DIMS = len(DIMC)\n",
        "\n",
        "print('done')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-15T22:04:26.371365Z",
          "iopub.execute_input": "2023-03-15T22:04:26.372355Z",
          "iopub.status.idle": "2023-03-15T22:04:36.169556Z",
          "shell.execute_reply.started": "2023-03-15T22:04:26.372296Z",
          "shell.execute_reply": "2023-03-15T22:04:36.168229Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3XL8evoqZP4",
        "outputId": "b9e38dff-4f92-4e5b-8066-ccdc3e1eed4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GET FEATURE FILES FROM GDRIVE \n",
        "# COLAB ONLY - MOVE FEATURE FILES TO WORKING DIRECTORY\n",
        "# \n",
        "import os\n",
        "import gc\n",
        "import shutil\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\") and not os.path.exists(WORKING_DIR):\n",
        "  !mkdir '/content/asl-work'\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "    shutil.copy(f\"/content/drive/MyDrive/GaggleSignLang/feature_data{FRAMES_OUT}.npy\", f\"{WORKING_DIR}\")\n",
        "    shutil.copy(f\"/content/drive/MyDrive/GaggleSignLang/feature_labels.npy\", f\"{WORKING_DIR}\")\n",
        "    shutil.copy(f\"/content/drive/MyDrive/GaggleSignLang/sign_to_prediction_index_map.json\", f\"{WORKING_DIR}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LYN0XSNKXd0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPOCESS-A\n",
        "datax = np.load(f\"{WORKING_DIR}/feature_data{FRAMES_OUT}.npy\")\n",
        "datay = np.load(f\"{WORKING_DIR}/feature_labels.npy\") \n",
        "\n",
        "data_rows = datax.shape[0]\n",
        "\n",
        "print(\"DATAX BASE\", datax.base)   #BASE1 ROOT READ FROM FILE\n",
        "print(\"datax shape\", datax.shape)\n",
        "# MAKE COPY OF DATAX1 input from file\n",
        "datax = datax.reshape((data_rows, FRAMES_OUT, -1, 3))[:,:,:,0:DIMS]  # BASE 1\n",
        "print(\"DATAX BASE\", datax.base[0,0,0]) \n",
        "\n",
        "# DEFINE VIEW MATRICES \n",
        "start_lips =  0\n",
        "start_left =  PRIM_HAND(\"WRIST\")\n",
        "start_pose =  POSE(\"NOSE\")\n",
        "start_right = SEC_HAND(\"WRIST\")\n",
        "end_right =   METALEN\n",
        "\n",
        "lips_3d =   datax[:, :, 0:PRIM_HAND(\"WRIST\"), :]\n",
        "lefth_3d =  datax[:, :, PRIM_HAND(\"WRIST\"):POSE(\"NOSE\"), :]\n",
        "pose_3d =   datax[:, :, POSE(\"NOSE\"):SEC_HAND(\"WRIST\"),:]\n",
        "righth_3d = datax[:, :, SEC_HAND(\"WRIST\"):METALEN, :]   # BASE1\n",
        "print(\"BASE RIGHT\", righth_3d.base[0,0,0])\n",
        "\n",
        "# Combind Componets to datax\n",
        "datax=np.concatenate((lips_3d, lefth_3d, pose_3d, righth_3d), axis=2)  ## COPY\n",
        "print(\"CONCATX BASE\", datax.base)  # BASE2 ROOT\n",
        "\n",
        "MIRROR = True\n",
        "if MIRROR: \n",
        "    # Mirror data on x-axis (max-value)\n",
        "    lips_m = lips_3d[:,:,:,0:DIMS]\n",
        "    lips_m[:,:,:,0] = np.nanmax(lips_3d[:,:,:,0]) - lips_3d[:,:,:,0]\n",
        "\n",
        "    left_m = lefth_3d[:,:,:,0:DIMS]\n",
        "    left_m[:,:,:,0] = np.nanmax(lefth_3d[:,:,:,0]) - lefth_3d[:,:,:,0]\n",
        "    pose_m = pose_3d[:,:,:,0:DIMS]\n",
        "    pose_m[:,:,:,0] = np.nanmax(pose_3d[:,:,:,0]) - pose_3d[:,:,:,0]\n",
        "    right_m = righth_3d[:,:,:,0:DIMS]\n",
        "    print(\"RIGHT_M BASE1\", right_m.base[0,0,0])\n",
        "    right_m[:,:,:,0] = np.nanmax(righth_3d[:,:,:,0]) - righth_3d[:,:,:,0]\n",
        "    print(\"RIGHT_M BASE2\", right_m.base[0,0,0])\n",
        "\n",
        "    # NOTE: reversed and swapped position of left_m and right_m \n",
        "    # so last slot is primary hand\n",
        "    datam = np.concatenate((lips_m, right_m, pose_m, left_m), axis=2)\n",
        "    print(\"DATAM\", datam.shape)\n",
        "\n",
        "    # find primary hand (Count NaNs)\n",
        "    #  -check whether lefth_3d OR righth_3d has more NaNs\n",
        "    cl = lefth_3d.reshape(lefth_3d.shape[0],-1)\n",
        "    cr = righth_3d.reshape(lefth_3d.shape[0],-1)\n",
        "    cc = np.isnan(cl).sum(axis=1) > np.isnan(cr).sum(axis=1)\n",
        "    datax[cc,:,:,:] = datam[cc,:,:,:]  # replace \n",
        "\n",
        "# REPLACE NaNs\n",
        "print('DATAX NANS FOR REPLACEMENT', np.isnan(datax).sum())\n",
        "#datax = np.nan_to_num(datax, copy=False)\n",
        "\n",
        "# SET 3D or 2D (SELECT dim columns)\n",
        "print(datax.shape)\n",
        "datax = datax[:,:,:,DIMC]\n",
        "\n",
        "print(\"FINAL SHAPE\")\n",
        "print(\"datax\", datax.shape)\n",
        "print(\"datay\", datay.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNWN8LmKx_gw",
        "outputId": "0d0cc0b4-bebe-461f-df94-fc80453e8087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATAX BASE None\n",
            "datax shape (94477, 24, 345)\n",
            "DATAX BASE 0.4689520001411438\n",
            "BASE RIGHT 0.4689520001411438\n",
            "CONCATX BASE None\n",
            "RIGHT_M BASE1 0.5005468130111694\n",
            "RIGHT_M BASE2 0.5005468130111694\n",
            "DATAM (94477, 24, 115, 3)\n",
            "DATAX NANS FOR REPLACEMENT 349448745\n",
            "(94477, 24, 115, 3)\n",
            "FINAL SHAPE\n",
            "datax (94477, 24, 115, 3)\n",
            "datay (94477,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datay = np.load(f\"{ARCHIVE_DIR}/feature_labels.npy\") \n",
        "datax = np.load(f\"{ARCHIVE_DIR}/feature_prep{FRAMES_OUT}npca.npy\") "
      ],
      "metadata": {
        "id": "gPT1QBsnT21b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distDiff(ds, ref, pts, use_ref=99):\n",
        "    if type(pts) is not list:\n",
        "      print(\"###ERROR: must pts must be a list, call with brackets '[pts]'\")\n",
        " \n",
        "    if use_ref==0:  # Diff from frame to frame (Out: [:,FRAMESOUT - 1,:,:)      \n",
        "        d = ds[:,1:, pts, :]  - ds[:,:-1, pts, :]\n",
        "\n",
        "    elif use_ref == 1:  # sumarize frame \n",
        "        #d1 = ds[:,:, pts, :].mean(axis=1)\n",
        "        #print(\"DD\", np.sum( np.linalg.norm(ds[:,1:, pts, :] - ds[:,:-1, pts, :], axis=3), axis=1).shape)\n",
        "        #print(\"D1\", np.nanmax(ds[:,:, pts, :], axis=1).shape)\n",
        "        d = np.hstack([np.nanmean(ds[:,:, pts, :], axis=1), \n",
        "                       np.nanmedian(ds[:,:, pts, :], axis=1), \n",
        "                       np.nanmax(ds[:,:, pts, :], axis=1), \n",
        "                       np.nanmin(ds[:,:, pts, :], axis=1),\n",
        "                       np.nanvar(ds[:,:, pts, :], axis=1)\n",
        "                       ]) \n",
        "        print('D!', d.shape)\n",
        "        d = np.expand_dims(d, axis=1)\n",
        "\n",
        "    else:   # points relative to reference (Out: [:, FRAMES_OUT,:,:)    \n",
        "        d = ds[:,:, pts, :] - ds[:,:, ref:(ref+1), :]\n",
        "        #d = d[:,:-1, :, :]\n",
        "\n",
        "\n",
        "    print(\"NEW REF\", d.shape)\n",
        "\n",
        "    #dshape = d.shape\n",
        "    d = d.reshape(d.shape[0], d.shape[1], -1)\n",
        "    \n",
        "    # NORMALIZE\n",
        "    #d = (d - np.nanmin(d, keepdims=True)) / (np.nanmax(d, keepdims=True) - np.nanmin(d, keepdims=True))\n",
        "    d = (d - np.nanmean(d, keepdims=True)) / np.nanstd(d, keepdims=True) # -1 to 1\n",
        "    return d\n",
        "\n",
        "\n",
        "d1 = distDiff(datax, PRIM_HAND(\"WRIST\"), \n",
        "              [PRIM_HAND(\"THUMB_TIP\"),\n",
        "               PRIM_HAND(\"INDEX_FINGER_TIP\"),\n",
        "               PRIM_HAND(\"MIDDLE_FINGER_TIP\"),\n",
        "               PRIM_HAND(\"RING_FINGER_TIP\"), \n",
        "               PRIM_HAND(\"PINKY_FINGER_TIP\"),\n",
        "               PRIM_HAND(\"THUMB_IP\"),\n",
        "               PRIM_HAND(\"INDEX_FINGER_PIP\"),\n",
        "               PRIM_HAND(\"MIDDLE_FINGER_PIP\"),\n",
        "               PRIM_HAND(\"RING_FINGER_PIP\"), \n",
        "               PRIM_HAND(\"PINKY_FINGER_PIP\")], 1)\n",
        "d2 = distDiff(datax, SEC_HAND(\"WRIST\"), \n",
        "              [SEC_HAND(\"THUMB_TIP\"),\n",
        "               SEC_HAND(\"INDEX_FINGER_TIP\"),\n",
        "               SEC_HAND(\"MIDDLE_FINGER_TIP\"),\n",
        "               SEC_HAND(\"RING_FINGER_TIP\"), \n",
        "               SEC_HAND(\"PINKY_FINGER_TIP\"),\n",
        "               SEC_HAND(\"THUMB_IP\"),\n",
        "               SEC_HAND(\"INDEX_FINGER_TIP\"),\n",
        "               SEC_HAND(\"MIDDLE_FINGER_TIP\"),\n",
        "               SEC_HAND(\"RING_FINGER_TIP\"), \n",
        "               SEC_HAND(\"PINKY_FINGER_TIP\")], 1)\n",
        "\n",
        "d3 = distDiff(datax, POSE(\"NOSE\"), \n",
        "              [POSE(\"LEFT_INDEX\"),\n",
        "               POSE(\"RIGHT_INDEX\"),\n",
        "               POSE(\"LEFT_WRIST\"),\n",
        "               POSE(\"RIGHT_WRIST\"),\n",
        "               POSE(\"LEFT_EAR\"),\n",
        "               POSE(\"RIGHT_EAR\"),\n",
        "               POSE(\"MOUTH_LEFT\"),\n",
        "               POSE(\"MOUTH_RIGHT\"),\n",
        "               POSE(\"RIGHT_ELBOW\"), \n",
        "               POSE(\"LEFT_ELBOW\")], 1)\n",
        "\n",
        "d4 = distDiff(datax, 5, list(range(0, 40, 4)), 1 ) # use 10 samples of mouth\n",
        "\n",
        "print(\"D1 phand NaNs\", d1.shape, np.isnan(d1).mean())\n",
        "print(\"D2 shand NaNs\", d2.shape, np.isnan(d2).mean())\n",
        "print(\"D3 pose NaNs\", d3.shape, np.isnan(d3).mean())\n",
        "print(\"D4 lips NaNs\", d4.shape, np.isnan(d4).mean())\n",
        "\n",
        "if d1.shape[1] > min(d1.shape[1], d2.shape[1], d3.shape[1], d4.shape[1]):\n",
        "  d1 = d1[:,:-1, :]\n",
        "if d2.shape[1] > min(d1.shape[1], d2.shape[1], d3.shape[1], d4.shape[1]):\n",
        "  d2 = d2[:,:-1, :]\n",
        "if d3.shape[1] > min(d1.shape[1], d2.shape[1], d3.shape[1], d4.shape[1]):\n",
        "  d3 = d3[:,:-1, :]\n",
        "if d4.shape[1] > min(d1.shape[1], d2.shape[1], d3.shape[1], d4.shape[1]):\n",
        "  d4 = d4[:,:-1, :]\n",
        "\n",
        "datax = np.concatenate((d1, d2, d3, d4), axis=2)\n",
        "print('DATAX NANS FOR REPLACEMENT', np.isnan(datax).sum())\n",
        "datax = np.nan_to_num(datax, copy=False)\n",
        "\n",
        "datax = datax.reshape(datax.shape[0], datax.shape[1],-1,DIMS)\n",
        "print(datax.shape)\n",
        "\n",
        "PTS_IN_FRAME = datax.shape[2]\n",
        "FRAMES_MODEL = datax.shape[1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sgZEC3UrX47",
        "outputId": "d21fca97-5076-4415-d861-5362d868eb26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D! (94477, 50, 3)\n",
            "NEW REF (94477, 1, 50, 3)\n",
            "D! (94477, 50, 3)\n",
            "NEW REF (94477, 1, 50, 3)\n",
            "D! (94477, 50, 3)\n",
            "NEW REF (94477, 1, 50, 3)\n",
            "D! (94477, 50, 3)\n",
            "NEW REF (94477, 1, 50, 3)\n",
            "D1 phand NaNs (94477, 1, 150) 0.0011960582999036803\n",
            "D2 shand NaNs (94477, 1, 150) 0.9608158599447485\n",
            "D3 pose NaNs (94477, 1, 150) 0.0\n",
            "D4 lips NaNs (94477, 1, 150) 0.0002751992548450946\n",
            "DATAX NANS FOR REPLACEMENT 13637100\n",
            "(94477, 1, 200, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelling\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Tree Visualisation\n",
        "from sklearn.tree import export_graphviz\n",
        "from IPython.display import Image\n",
        "import graphviz\n",
        "\n",
        "# Split the data into training and test sets\n",
        "trainx, testx, trainy, testy = train_test_split(datax.reshape(datax.shape[0], -1), datay, test_size=0.15, random_state=42)\n"
      ],
      "metadata": {
        "id": "AfFnh22EStqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ASLData(Dataset):\n",
        "    def __init__(self, datax, datay):\n",
        "        self.datax = datax\n",
        "        self.datay = datay\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.datax[index, :], self.datay[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.datay)\n",
        "\n",
        "# https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab\n",
        "class ASLModel(nn.Module):\n",
        "    def __init__(self, p):\n",
        "        super(ASLModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.batchnorm0 = nn.BatchNorm1d(FRAMES_MODEL * PTS_IN_FRAME * DIMS)\n",
        "        self.dropout0 = nn.Dropout(p)\n",
        "        \n",
        "        self.layer1 = nn.Linear(FRAMES_MODEL * PTS_IN_FRAME * DIMS, 512)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.layer2 = nn.Linear(512, 512)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.layerFC = nn.Linear(512, 250)\n",
        " \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        # x = self.batchnorm0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.layerFC(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "-BqkHm9-heOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############\n",
        "# !!! TRAINING DOES NOT RUN ON MAC OS - (cuda)\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"++++using GPU++++\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"++++using CPU++++\")\n",
        "\n",
        "EPOCHS = 40\n",
        "BATCH_SIZE = 64\n",
        "start_time = time.perf_counter()\n",
        "print(\"DATAX SHAPE IN\", datax.shape)\n",
        "\n",
        "#datax = datax.reshape(datax.shape[0],datax.shape[1], -1) #.swapaxes(1,2)\n",
        "print(\"DATAX SHAPE IN2\", datax.shape)\n",
        "datax = torch.tensor(datax)  # Convert to Torch Tensor\n",
        "\n",
        "trainx, testx, trainy, testy = train_test_split(datax, datay, test_size=0.15, random_state=42)\n",
        "\n",
        "train_data = ASLData(trainx, trainy)\n",
        "valid_data = ASLData(testx, testy)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)\n",
        "val_loader = DataLoader(valid_data, batch_size=BATCH_SIZE, num_workers=4, shuffle=False)\n",
        "\n",
        "model = ASLModel(0.2).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "sched = torch.optim.lr_scheduler.StepLR(opt, step_size=300, gamma=0.95)\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    model.train()\n",
        "    \n",
        "    train_loss_sum = 0.\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    train_bar = train_loader\n",
        "    for x,y in train_bar:\n",
        "        x = torch.Tensor(x).float().to(device)\n",
        "        y = torch.Tensor(y).long().to(device)  \n",
        "        y_pred = model(x)\n",
        "        \n",
        "        loss = criterion(y_pred, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        \n",
        "        train_loss_sum += loss.item()\n",
        "        train_correct += np.sum((np.argmax(y_pred.detach().cpu().numpy(), axis=1) == y.cpu().numpy()))\n",
        "        train_total += 1\n",
        "        sched.step()\n",
        "        \n",
        "    val_loss_sum = 0.\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    model.eval()\n",
        "    for x,y in val_loader:\n",
        "        x = torch.Tensor(x).float().to(device)\n",
        "        y = torch.Tensor(y).long().to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            val_loss_sum += loss.item()\n",
        "            val_correct += np.sum((np.argmax(y_pred.cpu().numpy(), axis=1) == y.cpu().numpy()))\n",
        "            val_total += 1\n",
        "    print(f\"DIM={DIMS} FRAMES={FRAMES_OUT}, FEAT={PTS_IN_FRAME}\")                          \n",
        "    print(f\"Epoch:{i} > Train Loss: {(train_loss_sum/train_total):.04f}, Train Acc: {train_correct/len(train_data):0.04f}\")\n",
        "    print(f\"Epoch:{i} > Val Loss: {(val_loss_sum/val_total):.04f}, Val Acc: {val_correct/len(valid_data):0.04f}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# Save the pytorch model\n",
        "py_model_path = f\"{ARCHIVE_DIR}/models/py_model.pt\"\n",
        "torch.save(model, py_model_path)\n",
        "\n",
        "print(\"#### ELAPSED TIME:\", time.perf_counter()-start_time)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-13T18:16:57.114241Z",
          "iopub.execute_input": "2023-03-13T18:16:57.115451Z",
          "iopub.status.idle": "2023-03-13T18:23:25.123968Z",
          "shell.execute_reply.started": "2023-03-13T18:16:57.115400Z",
          "shell.execute_reply": "2023-03-13T18:23:25.122556Z"
        },
        "trusted": true,
        "id": "RlbigS0EqZP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e87b6024-695b-48e0-c755-e45a3323a28f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++++using GPU++++\n",
            "DATAX SHAPE IN (94477, 1, 200, 3)\n",
            "DATAX SHAPE IN2 (94477, 1, 200, 3)\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:0 > Train Loss: 3.5700, Train Acc: 0.2009\n",
            "Epoch:0 > Val Loss: 2.6336, Val Acc: 0.3617\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:1 > Train Loss: 2.5021, Train Acc: 0.3845\n",
            "Epoch:1 > Val Loss: 2.2898, Val Acc: 0.4406\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:2 > Train Loss: 2.1744, Train Acc: 0.4549\n",
            "Epoch:2 > Val Loss: 1.8932, Val Acc: 0.5365\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:3 > Train Loss: 1.9560, Train Acc: 0.5012\n",
            "Epoch:3 > Val Loss: 1.8278, Val Acc: 0.5471\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:4 > Train Loss: 1.8037, Train Acc: 0.5355\n",
            "Epoch:4 > Val Loss: 1.6533, Val Acc: 0.5925\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:5 > Train Loss: 1.6870, Train Acc: 0.5609\n",
            "Epoch:5 > Val Loss: 1.5635, Val Acc: 0.6093\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:6 > Train Loss: 1.5948, Train Acc: 0.5825\n",
            "Epoch:6 > Val Loss: 1.5144, Val Acc: 0.6294\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:7 > Train Loss: 1.5204, Train Acc: 0.5993\n",
            "Epoch:7 > Val Loss: 1.4742, Val Acc: 0.6382\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:8 > Train Loss: 1.4615, Train Acc: 0.6119\n",
            "Epoch:8 > Val Loss: 1.4327, Val Acc: 0.6499\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:9 > Train Loss: 1.4072, Train Acc: 0.6263\n",
            "Epoch:9 > Val Loss: 1.4170, Val Acc: 0.6522\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:10 > Train Loss: 1.3653, Train Acc: 0.6364\n",
            "Epoch:10 > Val Loss: 1.3973, Val Acc: 0.6595\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:11 > Train Loss: 1.3383, Train Acc: 0.6436\n",
            "Epoch:11 > Val Loss: 1.3772, Val Acc: 0.6670\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:12 > Train Loss: 1.3076, Train Acc: 0.6497\n",
            "Epoch:12 > Val Loss: 1.3743, Val Acc: 0.6645\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:13 > Train Loss: 1.2827, Train Acc: 0.6542\n",
            "Epoch:13 > Val Loss: 1.3640, Val Acc: 0.6669\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:14 > Train Loss: 1.2679, Train Acc: 0.6588\n",
            "Epoch:14 > Val Loss: 1.3549, Val Acc: 0.6700\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:15 > Train Loss: 1.2510, Train Acc: 0.6619\n",
            "Epoch:15 > Val Loss: 1.3492, Val Acc: 0.6720\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:16 > Train Loss: 1.2390, Train Acc: 0.6645\n",
            "Epoch:16 > Val Loss: 1.3466, Val Acc: 0.6744\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:17 > Train Loss: 1.2325, Train Acc: 0.6671\n",
            "Epoch:17 > Val Loss: 1.3434, Val Acc: 0.6753\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:18 > Train Loss: 1.2194, Train Acc: 0.6700\n",
            "Epoch:18 > Val Loss: 1.3404, Val Acc: 0.6776\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:19 > Train Loss: 1.2126, Train Acc: 0.6710\n",
            "Epoch:19 > Val Loss: 1.3363, Val Acc: 0.6770\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:20 > Train Loss: 1.2143, Train Acc: 0.6715\n",
            "Epoch:20 > Val Loss: 1.3351, Val Acc: 0.6769\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:21 > Train Loss: 1.2102, Train Acc: 0.6723\n",
            "Epoch:21 > Val Loss: 1.3422, Val Acc: 0.6746\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:22 > Train Loss: 1.2018, Train Acc: 0.6747\n",
            "Epoch:22 > Val Loss: 1.3324, Val Acc: 0.6804\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:23 > Train Loss: 1.1980, Train Acc: 0.6766\n",
            "Epoch:23 > Val Loss: 1.3401, Val Acc: 0.6760\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:24 > Train Loss: 1.1955, Train Acc: 0.6768\n",
            "Epoch:24 > Val Loss: 1.3343, Val Acc: 0.6791\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:25 > Train Loss: 1.1951, Train Acc: 0.6768\n",
            "Epoch:25 > Val Loss: 1.3319, Val Acc: 0.6791\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:26 > Train Loss: 1.1949, Train Acc: 0.6771\n",
            "Epoch:26 > Val Loss: 1.3333, Val Acc: 0.6789\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:27 > Train Loss: 1.1940, Train Acc: 0.6762\n",
            "Epoch:27 > Val Loss: 1.3299, Val Acc: 0.6778\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:28 > Train Loss: 1.1890, Train Acc: 0.6792\n",
            "Epoch:28 > Val Loss: 1.3288, Val Acc: 0.6805\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:29 > Train Loss: 1.1909, Train Acc: 0.6773\n",
            "Epoch:29 > Val Loss: 1.3352, Val Acc: 0.6775\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:30 > Train Loss: 1.1915, Train Acc: 0.6754\n",
            "Epoch:30 > Val Loss: 1.3320, Val Acc: 0.6789\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:31 > Train Loss: 1.1921, Train Acc: 0.6771\n",
            "Epoch:31 > Val Loss: 1.3310, Val Acc: 0.6803\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:32 > Train Loss: 1.1856, Train Acc: 0.6788\n",
            "Epoch:32 > Val Loss: 1.3291, Val Acc: 0.6812\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:33 > Train Loss: 1.1916, Train Acc: 0.6778\n",
            "Epoch:33 > Val Loss: 1.3324, Val Acc: 0.6789\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:34 > Train Loss: 1.1888, Train Acc: 0.6766\n",
            "Epoch:34 > Val Loss: 1.3324, Val Acc: 0.6804\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:35 > Train Loss: 1.1900, Train Acc: 0.6782\n",
            "Epoch:35 > Val Loss: 1.3320, Val Acc: 0.6804\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:36 > Train Loss: 1.1916, Train Acc: 0.6759\n",
            "Epoch:36 > Val Loss: 1.3293, Val Acc: 0.6798\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:37 > Train Loss: 1.1843, Train Acc: 0.6813\n",
            "Epoch:37 > Val Loss: 1.3368, Val Acc: 0.6759\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:38 > Train Loss: 1.1861, Train Acc: 0.6806\n",
            "Epoch:38 > Val Loss: 1.3321, Val Acc: 0.6797\n",
            "==================================================\n",
            "DIM=3 FRAMES=64, FEAT=200\n",
            "Epoch:39 > Train Loss: 1.1878, Train Acc: 0.6780\n",
            "Epoch:39 > Val Loss: 1.3340, Val Acc: 0.6808\n",
            "==================================================\n",
            "#### ELAPSED TIME: 295.05017129000043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Results For Ensemble\n",
        "import pickle\n",
        "xx = torch.Tensor(testx).float().to(device)\n",
        "probc = model(xx).detach().cpu().numpy()\n",
        "print(type(probc), probc.shape)\n",
        "with open(f\"/content/drive/MyDrive/GaggleSignLang/prob_cnn.pkl\", 'wb') as f2:\n",
        "        pickle.dump(probc, f2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb5FBiuJnfol",
        "outputId": "09a62690-0b06-4f62-e3b8-9c90b51c4716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'> (14172, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####\n",
        "Normalization raises results ,03\n",
        "3d raises by .03  (.6300)\n",
        "\n",
        "# mean, median, max, min\n",
        "==================================================\n",
        "DIM=3 FRAMES=32, FEAT=200\n",
        "Epoch:37 > Train Loss: 1.1777, Train Acc: 0.6809\n",
        "Epoch:37 > Val Loss: 1.3139, Val Acc: 0.6799\n",
        "==================================================\n",
        "# mean, median, max, min\n",
        "==================================================\n",
        "DIM=3 FRAMES=24, FEAT=200\n",
        "Epoch:31 > Train Loss: 1.2013, Train Acc: 0.6772\n",
        "Epoch:31 > Val Loss: 1.3403, Val Acc: 0.6786\n",
        "\n",
        "# mean, median, max, min\n",
        "==================================================\n",
        "DIM=3 FRAMES=12, FEAT=200\n",
        "Epoch:34 > Train Loss: 1.2815, Train Acc: 0.6605\n",
        "Epoch:34 > Val Loss: 1.4214, Val Acc: 0.6618\n",
        "==================================================\n",
        "\n",
        "\n",
        "# Mean, max, min Same features as below\n",
        "==================================================\n",
        "DIM=3 FRAMES=12, FEAT=120\n",
        "Epoch:39 > Train Loss: 1.2951, Train Acc: 0.6563\n",
        "Epoch:39 > Val Loss: 1.4362, Val Acc: 0.6555\n",
        "==================================================\n",
        "\n",
        "\n",
        "BEST SO FAR 12 frames, 3d, -1 to 1 Norm before model\n",
        "  LIPS 8, \n",
        "  POSE(\"NOSE\",\"LEFT_INDEX\",\"RIGHT_INDEX\",\"LEFT_WRIST\",\n",
        "       \"RIGHT_WRIST\",\"LEFT_EAR\",\"RIGHT_EAR\",\"MOUTH_LEFT\",\n",
        "       \"MOUTH_RIGHT\",\"RIGHT_ELBOW\",\"LEFT_ELBOW\"\n",
        "  HANDS (\"WRIST\"),\"THUMB_TIP\",\"INDEX_FINGER_TIP\",\"MIDDLE_FINGER_TIP\",\n",
        "        \"RING_FINGER_TIP\",\"PINKY_FINGER_TIP\", \"THUMB_IP\",\"INDEX_FINGER_MIP\",\"MIDDLE_FINGER_MIP\",\n",
        "        \"RING_FINGER_MIP\",\"PINKY_FINGER_MIP\"\n",
        "==================================================\n",
        "DIM=3 FRAMES=12, FEAT=40\n",
        "Epoch:39 > Train Loss: 1.1315, Train Acc: 0.7013\n",
        "Epoch:39 > Val Loss: 1.5225, Val Acc: 0.6375=\n",
        "\n",
        "==================================================\n",
        "DIM=3 FRAMES=12, FEAT=30\n",
        "Epoch:39 > Train Loss: 1.2799, Train Acc: 0.6651\n",
        "Epoch:39 > Val Loss: 1.6108, Val Acc: 0.6141\n",
        "==================================================\n",
        "\n",
        "Adding thumbs made it worse, adding pinkies worse\n",
        "==================================================\n",
        "DIM=3 FRAMES=12, FEAT=30\n",
        "Epoch:30 > Train Loss: 1.2816, Train Acc: 0.6646\n",
        "Epoch:30 > Val Loss: 1.6350, Val Acc: 0.6052\n",
        "==================================================\n",
        "\n",
        "ADD LIPS 8, ADD 0-24 POSE  (WORSE THAN selecte Pose)\n",
        "New zero center before / no norm in model  /NaN at end\n",
        "==================================================\n",
        "DIM=3 FRAMES=12, FEAT=43\n",
        "Epoch:21 > Train Loss: 1.6057, Train Acc: 0.5934\n",
        "Epoch:21 > Val Loss: 1.8616, Val Acc: 0.5565\n",
        "==================================================\n",
        "\n",
        "ADD LIPS 8\n",
        "New zero center before / no norm in model  /NaN at end\n",
        "==================================================\n",
        "DIM=3 FRAMES=12, FEAT=28\n",
        "Epoch:39 > Train Loss: 1.2635, Train Acc: 0.6697\n",
        "Epoch:39 > Val Loss: 1.6147, Val Acc: 0.6125\n",
        "==================================================\n",
        "\n",
        "ADD LIPS 10\n",
        "New zero center before / no norm in model  /NaN at end\n",
        "==================================================\n",
        "DIM=3 FRAMES=12, FEAT=30\n",
        "Epoch:38 > Train Loss: 1.2721, Train Acc: 0.6673\n",
        "Epoch:38 > Val Loss: 1.6027, Val Acc: 0.6135\n",
        "==================================================\n",
        "\n",
        "ADD LIPS 20\n",
        "New zero center before / no norm in model  /NaN at end\n",
        "==================================================\n",
        "DIM=3 FRAMES=12, FEAT=40\n",
        "Epoch:33 > Train Loss: 1.3318, Train Acc: 0.6543\n",
        "Epoch:33 > Val Loss: 1.6179, Val Acc: 0.6111\n",
        "==================================================\n",
        "\n",
        "ADD LIPS 40\n",
        "New zero center before / no norm in model  /NaN at end\n",
        "==================================================\n",
        "DIM=3 FRAMES=12, FEAT=60\n",
        "Epoch:29 > Train Loss: 1.4127, Train Acc: 0.6351\n",
        "Epoch:29 > Val Loss: 1.6383, Val Acc: 0.6038\n",
        "==================================================\n",
        "\n",
        "ADD LIPS\n",
        "New zero center before / no norm in model  /NaN at end\n",
        "==================================================\n",
        "DIM=3 FRAMES=8, FEAT=60\n",
        "Epoch:39 > Train Loss: 1.4758, Train Acc: 0.6228\n",
        "Epoch:39 > Val Loss: 1.6906, Val Acc: 0.5967\n",
        "==================================================\n",
        "\n",
        "ADD LIPS\n",
        "New zero center before / no norm in model  /NaN at end\n",
        "==================================================\n",
        "DIM=3 FRAMES=16, FEAT=60\n",
        "Epoch:39 > Train Loss: 1.4748, Train Acc: 0.6208\n",
        "Epoch:39 > Val Loss: 1.6669, Val Acc: 0.5979\n",
        "==================================================\n",
        "\n",
        "New zero center before / no norm in model  /NaN at end\n",
        "==================================================\n",
        "DIM=3 FRAMES=16, FEAT=20\n",
        "Epoch:22 > Train Loss: 1.4953, Train Acc: 0.6129\n",
        "Epoch:22 > Val Loss: 1.8729, Val Acc: 0.5503\n",
        "==================================================\n",
        "\n",
        "\n",
        "No Norm in/before model / NaN at end\n",
        "==================================================\n",
        "DIM=3 FRAMES=16, FEAT=20\n",
        "Epoch:25 > Train Loss: 1.7384, Train Acc: 0.5545\n",
        "Epoch:25 > Val Loss: 1.9828, Val Acc: 0.5224\n",
        "==================================================\n",
        "\n",
        "No Norm in model only / norm before, Nan at end\n",
        "==================================================\n",
        "DIM=3 FRAMES=16, FEAT=20\n",
        "Epoch:39 > Train Loss: 2.3872, Train Acc: 0.4160\n",
        "Epoch:39 > Val Loss: 2.4648, Val Acc: 0.4231\n",
        "==================================================\n",
        "\n",
        "Norm in model only, Nan at end\n",
        "==================================================\n",
        "DIM=3 FRAMES=16, FEAT=20\n",
        "Epoch:39 > Train Loss: 1.5367, Train Acc: 0.5995\n",
        "Epoch:39 > Val Loss: 1.9235, Val Acc: 0.5489\n",
        "==================================================\n",
        "\n",
        "Norm in  and before  model, nam at end\n",
        "==================================================\n",
        "DIM=3 FRAMES=16, FEAT=20\n",
        "Epoch:39 > Train Loss: 2.1667, Train Acc: 0.4613\n",
        "Epoch:39 > Val Loss: 2.3030, Val Acc: 0.4620\n",
        "==================================================\n",
        "#### ELAPSED TIME: 868.1053221210004"
      ],
      "metadata": {
        "id": "mzDryIEZoP4a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}