{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e94d6faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Jupiter  MacOS\n",
    "BASE_DIR = \"/Users/johnhanratty/ASLtest/asl-signs\"  #\"/Users/johnhanratty/ASLtest/asl-signs\"\n",
    "WORKING_DIR = \"/Users/johnhanratty/ASLtest\"\n",
    "ARCHIVE_DIR = \"/Users/johnhanratty/ASLtest\"\n",
    "MODEL_DIR = \"/Users/johnhanratty/ASLtest/models\"\n",
    "\n",
    "# !pip install nb_black --quiet\n",
    "# %load_ext lab_black\n",
    "\n",
    "# Colab\n",
    "# BASE_DIR = \"/content/asl-signs\"   #\"/content/drive/MyDrive/GaggleSignLang/asl-signs\"\n",
    "# WORKING_DIR = \"/content/asl-work\"\n",
    "# ARCHIVE_DIR = \"/content/drive/MyDrive/GaggleSignLang\"\n",
    "# MODEL_DIR = \"/content/drive/MyDrive/GaggleSignLang\"\n",
    "# !pip install nb_black --quiet\n",
    "# print('-----ok')\n",
    "# %load_ext nb_black\n",
    "\n",
    "# KAGGLE\n",
    "# BASE_DIR = \"/kaggle/input/asl-signs\"\n",
    "# WORKING_DIR = \"/kaggle/working\"\n",
    "# ARCHIVE_DIR = \"/kaggle/working\"\n",
    "# MODEL_DIR  = \"/kaggle/working\"\n",
    "# !pip install nb_black --quiet --root-user-action=ignore\n",
    "# %load_ext lab_black\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from random import seed, sample\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "LANDMARK_FILES_DIR = f'{BASE_DIR}/train_landmark_files'\n",
    "TRAIN_FILE = f\"{BASE_DIR}/train.csv\"\n",
    "\n",
    "FRAMES_OUT = 32 # 16\n",
    "PTS_IN_FRAME = 345\n",
    "DIMC = [0,1,2]\n",
    "DIMS = len(DIMC)\n",
    "WORKERS = 0   # dataoader work var  0 for MAC, 4 for online\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415cbd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e6dad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Len 675 165 165 180 165\n"
     ]
    }
   ],
   "source": [
    "# CNN TORCH FEATUREGEN MODEL \n",
    "ROWS_PER_FRAME = 543  # combined face, lefth, pose, righth\n",
    "PR_PTS = [40, 44, 48, 52, 56, 60, 43, 46, 50, 54, 58]\n",
    "SC_PTS = [40, 98, 102, 106, 110, 114, 97, 102, 106, 110, 114]\n",
    "PO_PTS = [60, 73, 80, 81, 76, 77, 68, 69, 70, 71, 75, 74]\n",
    "LI_PTS = [5, 0, 4, 8, 12, 16, 20, 24, 28, 32, 36]\n",
    "PR_LEN = len(PR_PTS) * DIMS * 5  # 5 = number of aggregations e.g. max,min\n",
    "SC_LEN = len(SC_PTS) * DIMS * 5\n",
    "PO_LEN = len(PO_PTS) * DIMS * 5\n",
    "LI_LEN = len(LI_PTS) * DIMS * 5\n",
    "CNN_FEAT_LEN = PR_LEN + SC_LEN + PO_LEN + LI_LEN \n",
    "\n",
    "print(\"Feature Len\", CNN_FEAT_LEN, PR_LEN, SC_LEN, PO_LEN, LI_LEN)\n",
    "\n",
    "# FILTER FEATURES IN EACH FRAME  - FACE, POSE & HANDs\n",
    "class FeatureGen(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureGen, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x)\n",
    "        \n",
    "        # FILTER TO SPECIFIED FRAMES (FRAMES_OUT)\n",
    "        seed(24)\n",
    "        n_frames = x.size()[0]\n",
    "        # Trim to # of frames to FRAMES_OUT\n",
    "        if n_frames > FRAMES_OUT:\n",
    "            idx = sorted((sample(range(0, n_frames), FRAMES_OUT)))\n",
    "            x=x[idx]\n",
    "        n_frames = x.size()[0]\n",
    "        # FLATTENING ROWS BY TYPE and CONCATENATING TO ONE ROW PER FRAME 3D (XYZ)\n",
    "        # INPUT NUMPY, TORCH OUTPUT\n",
    "\n",
    "        # The Video contains n_frames each containing exactly ROWS_PER_FRAME (543)frames.\n",
    "        # The frames in each are in order of feature type.\n",
    "        # The rows conain x, y, z for a feature\n",
    "        #   Video Format = [n_frames][543 frames][3 xyz coordinates]\n",
    "        \n",
    "        # Create views by data type (e.g. one point on hand) \n",
    "        # by selecting rows for each frame\n",
    "        # face_x = x[:,:468,:].contiguous().view(-1, 468*3)\n",
    "        lips_idx = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291, 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308, 95, 88, 178, 87, 14, 317, 402, 318, 324, 146, 91, 181, 84, 17, 314, 405, 321, 375]\n",
    "        lips_x = x[:, lips_idx,:].contiguous().view(-1, len(lips_idx)*3)\n",
    "        lefth_x = x[:,468:489,:].contiguous().view(-1, 21*3)\n",
    "        pose_x = x[:,489:522,:].contiguous().view(-1, 33*3)\n",
    "        righth_x = x[:,522:,:].contiguous().view(-1, 21*3)\n",
    "\n",
    "        # Check for primary hand and if left hand swap and rotate hands\n",
    "        if torch.isnan(lefth_x).sum() < torch.isnan(righth_x).sum():\n",
    "            prime_x = lefth_x\n",
    "            second_x = righth_x\n",
    "        else:\n",
    "            prime_x = righth_x.reshape(righth_x.size()[0], -1, DIMS)\n",
    "            prime_x[:,:,0] = torch.from_numpy(np.subtract(np.nanmax(prime_x[:,:,0].numpy(), axis=1).reshape(-1,1),\n",
    "                                    prime_x[:, :, 0].numpy()))\n",
    "            prime_x = prime_x.reshape(prime_x.size()[0],-1)\n",
    "            \n",
    "            second_x = lefth_x.reshape(lefth_x.size()[0], -1, DIMS)\n",
    "            second_x[:,:,0] = torch.from_numpy(np.subtract(np.nanmax(second_x[:,:,0].numpy(), axis=1).reshape(-1,1),\n",
    "                                          second_x[:, :, 0].numpy()))\n",
    "            second_x = second_x.reshape(second_x.size()[0],-1)\n",
    "            \n",
    "        \n",
    "        # create video withfixed number of frames (FRAMES_OUT)\n",
    "        # initialize with NoN so later operations can ignore them (e.g. nanmean()) \n",
    "        xfeat = torch.full([FRAMES_OUT, PTS_IN_FRAME], np.nan)\n",
    "        \n",
    "        # center frames\n",
    "        offset = (FRAMES_OUT - n_frames) // 2  # center frames in output data in each frame in video\n",
    "        \n",
    "        # flatten types into one row per frame\n",
    "        xfeat[offset:n_frames+offset,:] = torch.cat([lips_x, prime_x, pose_x, second_x], axis=1)  # concatenate types\n",
    "        \n",
    "        ############# CNN Specific\n",
    "        ############# \n",
    "        \n",
    "        def distDiff(ds, ref, pts):\n",
    "            ds = ds.reshape(ds.shape[0],  -1, DIMS)\n",
    "            d = torch.hstack([torch.from_numpy(np.nanmean(ds[:, pts, :].numpy(), axis=0)), \n",
    "                           torch.from_numpy( np.nanmedian(ds[:, pts, :].numpy(), axis=0)), \n",
    "                           torch.from_numpy(np.nanmax(ds[:, pts, :].numpy(), axis=0)), \n",
    "                           torch.from_numpy(np.nanmin(ds[:, pts, :].numpy(), axis=0)),\n",
    "                           torch.from_numpy(np.nanvar(ds[:, pts, :].numpy(), axis=0))\n",
    "                           ]) \n",
    "            d = d.reshape(1, -1) \n",
    "            # NORMALIZE\n",
    "           # d = (d - np.nanmean(d, keepdims=True)) / np.nanstd(d, keepdims=True) # -1 to 1\n",
    "            d = np.nan_to_num(d, copy=False)  # replace NaN after normalization\n",
    "            return d\n",
    "        \n",
    "        d1 = distDiff(xfeat, 40, [40, 44, 48, 52, 56, 60, 43, 46, 50, 54, 58])\n",
    "        d2 = distDiff(xfeat, 40, [40,98, 102, 106, 110, 114, 97, 102, 106, 110, 114])\n",
    "        d3 = distDiff(xfeat, 60, [60, 73, 80, 81, 76, 77, 68, 69, 70, 71, 75, 74])\n",
    "        d4 = distDiff(xfeat, 5,  [5,0, 4, 8, 12, 16, 20, 24, 28, 32, 36])\n",
    "        \n",
    "        return np.concatenate([d1,d2,d3,d4], axis=1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d37e8ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert&Save (94477, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 94477/94477 [13:33<00:00, 116.08it/s]\n"
     ]
    }
   ],
   "source": [
    "## PROCESS EACH ROW (ONE PARQUET PER ROW)\n",
    "def convert_row(row):\n",
    "    x = load_relevant_data_subset(os.path.join(BASE_DIR, row[1].path))\n",
    "    x = feature_converter(torch.tensor(x))\n",
    "    return x, row[1].label\n",
    "\n",
    "## LOOP THROUGH PARQUET FILES LISTED IN TRAIN FILE\n",
    "##  SAVE RESULTS \n",
    "def convert_and_save_data():\n",
    "    label_map = json.load(open(f\"{BASE_DIR}/sign_to_prediction_index_map.json\", \"r\"))\n",
    "    df = pd.read_csv(TRAIN_FILE)\n",
    "    df['label'] = df['sign'].map(label_map)\n",
    "    \n",
    "    print(\"Convert&Save\", df.shape)\n",
    "    #### FOR TESTING #################\n",
    "    #df = df[0:20]\n",
    "    ##################################\n",
    "\n",
    "    npdata = np.zeros((df.shape[0], CNN_FEAT_LEN))  \n",
    "\n",
    "    nplabels = np.zeros(df.shape[0])\n",
    "    \n",
    "    results = map(convert_row, df.iterrows())\n",
    "    for i, (x,y) in tqdm(enumerate(results), total=df.shape[0]):\n",
    "            npdata[i,:] = x\n",
    "            nplabels[i] = y\n",
    "    return npdata, nplabels\n",
    " \n",
    "\n",
    "feature_converter = FeatureGen()\n",
    "datax, datay = convert_and_save_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebdde79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "np.save(f\"{ARCHIVE_DIR}/cnn_data{FRAMES_OUT}.npy\", datax)\n",
    "np.save(f\"{ARCHIVE_DIR}/cnn_labels.npy\", datay)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d1c8fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL\n",
    "### NEW SEPARATED INPUTS\n",
    "class ASLData(Dataset):\n",
    "    def __init__(self,datax,datay):\n",
    "        self.datax = datax\n",
    "        self.datay = datay\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.datax[index, :], self.datay[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datay)\n",
    "\n",
    "# https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab\n",
    "class ASLModel(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(ASLModel, self).__init__()\n",
    "        \n",
    "        # DATA in [1, CNN_FEAT_LEN] per video\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        L1OUT = 512  #1024 was ok\n",
    "        L2OUT = 512\n",
    "\n",
    "        self.layer_ph = nn.Linear(PR_LEN, L1OUT)\n",
    "        self.batchnorm_ph = nn.BatchNorm1d(L1OUT)\n",
    "        \n",
    "        self.layer_sh = nn.Linear(SC_LEN, L1OUT)\n",
    "        self.batchnorm_sh = nn.BatchNorm1d(L1OUT)\n",
    " \n",
    "        self.layer_po = nn.Linear(PO_LEN, L1OUT)\n",
    "        self.batchnorm_po = nn.BatchNorm1d(L1OUT)\n",
    " \n",
    "        self.layer_li = nn.Linear(LI_LEN, L1OUT)\n",
    "        self.batchnorm_li = nn.BatchNorm1d(L1OUT) \n",
    " \n",
    "        self.layer1 = nn.Linear(4*L1OUT, L2OUT)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(L2OUT)\n",
    "\n",
    "        self.layerFC = nn.Linear(L2OUT, 250)\n",
    "        self.softmax = nn.Softmax()\n",
    " \n",
    "        \n",
    "    def forward(self, x):\n",
    "        phand = x[:,0:PR_LEN]            \n",
    "        shand = x[:,PR_LEN:(PR_LEN+PR_LEN)]\n",
    "        pose =  x[:,(PR_LEN+PR_LEN):(PR_LEN+PR_LEN+PO_LEN)]  \n",
    "        lips =  x[:,(PR_LEN+PR_LEN+PO_LEN):CNN_FEAT_LEN]  \n",
    "        \n",
    "        ph = self.flatten(torch.tensor(phand).float()) \n",
    "        ph = self.layer_ph(ph)\n",
    "        ph = self.batchnorm_ph(ph)\n",
    "        ph = self.relu(ph)\n",
    "        ph = self.dropout(ph)\n",
    "\n",
    "        sh = self.flatten(torch.tensor(shand).float())       \n",
    "        sh = self.layer_sh(sh)\n",
    "        sh = self.batchnorm_sh(sh)\n",
    "        sh = self.relu(sh)\n",
    "        sh = self.dropout(sh)\n",
    "       \n",
    "        po = self.flatten(torch.tensor(pose).float())       \n",
    "        po = self.layer_po(po)\n",
    "        po = self.batchnorm_po(po)\n",
    "        po = self.relu(po)\n",
    "        po = self.dropout(po)\n",
    "        \n",
    "        li = self.flatten(torch.tensor(lips).float())       \n",
    "        li = self.layer_li(li)\n",
    "        li = self.batchnorm_li(li)\n",
    "        li = self.relu(li)\n",
    "        li = self.dropout(li)\n",
    "\n",
    "        x = torch.cat((ph.view(ph.size(0), -1),\n",
    "                       sh.view(sh.size(0), -1),\n",
    "                       po.view(po.size(0), -1),\n",
    "                       li.view(li.size(0), -1)), dim=1)\n",
    "        # x = self.batchnorm0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layerFC(x)\n",
    "       # x = self.softmax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "222d024b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++using CPU++++\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:0 > Train Loss: 3.4657, Train Acc: 0.2211\n",
      "Epoch:0 > Val Loss: 2.7225, Val Acc: 0.3496\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:1 > Train Loss: 2.3572, Train Acc: 0.4167\n",
      "Epoch:1 > Val Loss: 2.0977, Val Acc: 0.4899\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:2 > Train Loss: 2.0062, Train Acc: 0.4925\n",
      "Epoch:2 > Val Loss: 1.8331, Val Acc: 0.5407\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:3 > Train Loss: 1.7816, Train Acc: 0.5415\n",
      "Epoch:3 > Val Loss: 1.7013, Val Acc: 0.5789\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:4 > Train Loss: 1.6070, Train Acc: 0.5849\n",
      "Epoch:4 > Val Loss: 1.6092, Val Acc: 0.5973\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:5 > Train Loss: 1.4838, Train Acc: 0.6126\n",
      "Epoch:5 > Val Loss: 1.5411, Val Acc: 0.6128\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:6 > Train Loss: 1.3765, Train Acc: 0.6388\n",
      "Epoch:6 > Val Loss: 1.4306, Val Acc: 0.6447\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:7 > Train Loss: 1.2991, Train Acc: 0.6574\n",
      "Epoch:7 > Val Loss: 1.3641, Val Acc: 0.6636\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:8 > Train Loss: 1.2320, Train Acc: 0.6725\n",
      "Epoch:8 > Val Loss: 1.3221, Val Acc: 0.6766\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:9 > Train Loss: 1.1759, Train Acc: 0.6869\n",
      "Epoch:9 > Val Loss: 1.3021, Val Acc: 0.6806\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:10 > Train Loss: 1.1303, Train Acc: 0.6968\n",
      "Epoch:10 > Val Loss: 1.2850, Val Acc: 0.6903\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:11 > Train Loss: 1.0968, Train Acc: 0.7042\n",
      "Epoch:11 > Val Loss: 1.2725, Val Acc: 0.6892\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:12 > Train Loss: 1.0657, Train Acc: 0.7116\n",
      "Epoch:12 > Val Loss: 1.2609, Val Acc: 0.6930\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:13 > Train Loss: 1.0452, Train Acc: 0.7179\n",
      "Epoch:13 > Val Loss: 1.2603, Val Acc: 0.6946\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:14 > Train Loss: 1.0254, Train Acc: 0.7220\n",
      "Epoch:14 > Val Loss: 1.2462, Val Acc: 0.6978\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:15 > Train Loss: 1.0138, Train Acc: 0.7248\n",
      "Epoch:15 > Val Loss: 1.2472, Val Acc: 0.6976\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:16 > Train Loss: 0.9927, Train Acc: 0.7300\n",
      "Epoch:16 > Val Loss: 1.2374, Val Acc: 0.6991\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:17 > Train Loss: 0.9843, Train Acc: 0.7322\n",
      "Epoch:17 > Val Loss: 1.2359, Val Acc: 0.6993\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:18 > Train Loss: 0.9741, Train Acc: 0.7349\n",
      "Epoch:18 > Val Loss: 1.2317, Val Acc: 0.7017\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:19 > Train Loss: 0.9732, Train Acc: 0.7343\n",
      "Epoch:19 > Val Loss: 1.2329, Val Acc: 0.7015\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:20 > Train Loss: 0.9658, Train Acc: 0.7373\n",
      "Epoch:20 > Val Loss: 1.2327, Val Acc: 0.7024\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:21 > Train Loss: 0.9576, Train Acc: 0.7396\n",
      "Epoch:21 > Val Loss: 1.2325, Val Acc: 0.7017\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:22 > Train Loss: 0.9562, Train Acc: 0.7412\n",
      "Epoch:22 > Val Loss: 1.2340, Val Acc: 0.7024\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:23 > Train Loss: 0.9506, Train Acc: 0.7412\n",
      "Epoch:23 > Val Loss: 1.2280, Val Acc: 0.7029\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:24 > Train Loss: 0.9496, Train Acc: 0.7415\n",
      "Epoch:24 > Val Loss: 1.2388, Val Acc: 0.7011\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:25 > Train Loss: 0.9469, Train Acc: 0.7447\n",
      "Epoch:25 > Val Loss: 1.2361, Val Acc: 0.7027\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:26 > Train Loss: 0.9431, Train Acc: 0.7441\n",
      "Epoch:26 > Val Loss: 1.2279, Val Acc: 0.7015\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:27 > Train Loss: 0.9467, Train Acc: 0.7417\n",
      "Epoch:27 > Val Loss: 1.2280, Val Acc: 0.7028\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:28 > Train Loss: 0.9444, Train Acc: 0.7422\n",
      "Epoch:28 > Val Loss: 1.2302, Val Acc: 0.7019\n",
      "==================================================\n",
      "DIM=3 FRAMES=32, FEAT=345\n",
      "Epoch:29 > Train Loss: 0.9422, Train Acc: 0.7439\n",
      "Epoch:29 > Val Loss: 1.2324, Val Acc: 0.7019\n",
      "==================================================\n",
      "   truth  cnn\n",
      "0  206.0   78\n",
      "1   20.0   96\n",
      "2  178.0  222\n",
      "3  114.0  114\n",
      "4  221.0  221\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/GaggleSignLang/pred_cnn.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m pred_list[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(prob_cnn, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred_list\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/GaggleSignLang/pred_cnn.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f1:\n\u001b[1;32m     83\u001b[0m        pickle\u001b[38;5;241m.\u001b[39mdump(pred_cnn, f1)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/GaggleSignLang/prob_cnn.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f1:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/GaggleSignLang/pred_cnn.pkl'"
     ]
    }
   ],
   "source": [
    "## MULTI TRAINING\n",
    "# !!! TRAINING DOES NOT RUN ON MAC OS - (cuda)\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  print(\"++++using GPU++++\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print(\"++++using CPU++++\")\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "#datax = datax.reshape(datax.shape[0],datax.shape[1], -1) #.swapaxes(1,2)\n",
    "#datax = torch.tensor(datax)  # Convert to Torch Tensor\n",
    "datax = torch.tensor(datax)  # Convert to Torch Tensor\n",
    "\n",
    "trainx, testx, trainy, testy = train_test_split(datax, datay, test_size=0.15, random_state=42)\n",
    "\n",
    "# init list for saving predictions for ensemble processing\n",
    "pred_list = pd.DataFrame(testy, columns=[\"truth\"])\n",
    "\n",
    "train_data = ASLData(trainx, trainy)\n",
    "valid_data = ASLData(testx, testy)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=WORKERS, shuffle=True)\n",
    "val_loader = DataLoader(valid_data, batch_size=BATCH_SIZE, num_workers=WORKERS, shuffle=False)\n",
    "model = ASLModel(0.2).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "sched = torch.optim.lr_scheduler.StepLR(opt, step_size=300, gamma=0.95)\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss_sum = 0.\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_bar = train_loader\n",
    "    for x,y in train_bar:\n",
    "        x = torch.Tensor(x).float().to(device)\n",
    "        y = torch.Tensor(y).long().to(device) \n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        train_loss_sum += loss.item()\n",
    "        train_correct += np.sum((np.argmax(y_pred.detach().cpu().numpy(), axis=1) == y.cpu().numpy()))\n",
    "        train_total += 1\n",
    "        sched.step()\n",
    "        \n",
    "    val_loss_sum = 0.\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    model.eval()\n",
    "    for x,y in val_loader:\n",
    "        x = torch.Tensor(x).float().to(device)\n",
    "        y = torch.Tensor(y).long().to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            val_loss_sum += loss.item()\n",
    "            val_correct += np.sum((np.argmax(y_pred.cpu().numpy(), axis=1) == y.cpu().numpy()))\n",
    "            val_total += 1\n",
    "    print(f\"DIM={DIMS} FRAMES={FRAMES_OUT}, FEAT={PTS_IN_FRAME}\")                          \n",
    "    print(f\"Epoch:{i} > Train Loss: {(train_loss_sum/train_total):.04f}, Train Acc: {train_correct/len(train_data):0.04f}\")\n",
    "    print(f\"Epoch:{i} > Val Loss: {(val_loss_sum/val_total):.04f}, Val Acc: {val_correct/len(valid_data):0.04f}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Save the pytorch model\n",
    "PATH = f\"{ARCHIVE_DIR}/models/modelccn{FRAMES_OUT}.sd\"\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "x = testx.detach().numpy()\n",
    "prob_cnn = model(torch.tensor(x)).detach().numpy()\n",
    "\n",
    "pred_list['cnn'] = np.argmax(prob_cnn, axis=1)\n",
    "print(pred_list.head())\n",
    "\n",
    "with open(f\"/content/drive/MyDrive/GaggleSignLang/pred_cnn.pkl\", 'wb') as f1:\n",
    "       pickle.dump(pred_cnn, f1)\n",
    "with open(f\"/content/drive/MyDrive/GaggleSignLang/prob_cnn.pkl\", 'wb') as f1:\n",
    "       pickle.dump(prob_cnn, f1)\n",
    "print(prob_cnn.shape)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", np.mean(pred_list.truth == pred_list.cnn))\n",
    "print(\"#### ELAPSED TIME:\", time.perf_counter()-start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0acf8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
